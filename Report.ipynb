{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset comes from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/spambase). It consists in 4,601 sms messages that are described as either spam or ham. Each message is characterized by 57 continuous features. The 58th is the classification as spam or ham. There are 1813 (39.4%) messages classified as spam and 2788 (60.6%) as non-spam.\n",
    "\n",
    "The first 48 columns represent the frequency of specific words. Values are real and their range spans from 0 to 100.\n",
    "The following 6 columns depict the frequency of specific character. Values are also real and have the same span than the before ones.\n",
    "The last 3 columns display some statistics about the capital letters. Values are real and their range spans from 0 to (+)infinity.\n",
    "\n",
    "More information on this dataset can be found in files \"spambase.DOCUMENTATION\" and \"spambase.names\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "data = pd.read_csv('spambase.data').as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, a naive Bayes model is used to classify the sms messages. This model will be useful to assess the performance of future ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without K-fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Importing libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library MultinomialNB from sklearn is used to run a Naive Bayes classification.\n",
    "It accepts number values as input and not text data, which is ok as our data consists in 57 features of continuous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Splitting into training and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is split into training and testing sets with a 80-20 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain,Xtest,Ytrain,Ytest = train_test_split(data[:,0:56], data[:,57], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification rate for NB: 0.842391304348\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Naive Bayes\n",
    "model = MultinomialNB()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"Classification rate for NB:\", model.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of one run of multinomial naive Bayes already depicts good result.\n",
    "To be sure it is not a lucky shot, a K-fold multinomial naive Bayes is performed just after."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification rate for NB: 0.865217391304\n",
      "Classification rate for NB: 0.839130434783\n",
      "Classification rate for NB: 0.823913043478\n",
      "Classification rate for NB: 0.810869565217\n",
      "Classification rate for NB: 0.85652173913\n",
      "Classification rate for NB: 0.867391304348\n",
      "Classification rate for NB: 0.832608695652\n",
      "Classification rate for NB: 0.839130434783\n",
      "Classification rate for NB: 0.819565217391\n",
      "Classification rate for NB: 0.830434782609\n",
      "The overall mean rate is: 0.83847826087\n"
     ]
    }
   ],
   "source": [
    "# K-fold\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "score_NB = 0\n",
    "for train_index, test_index in kf.split(data):\n",
    "    Xtrain, Xtest = data[train_index,0:56], data[test_index,0:56]\n",
    "    Ytrain, Ytest = data[train_index,57], data[test_index,57]\n",
    "    \n",
    "    # Multinomial Naive Bayes\n",
    "    model = MultinomialNB()\n",
    "    model.fit(Xtrain, Ytrain)\n",
    "    score = model.score(Xtest, Ytest)\n",
    "    score_NB += score\n",
    "    print(\"Classification rate for NB:\", score)\n",
    "score_NB /= kf.get_n_splits()\n",
    "print(\"The overall mean rate is:\",score_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall mean score is quite close to the one without the K-fold, which proves that results of multinomial naive Bayes are quite resilient for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An artificial neural network (ANN) is then run to classify the sms messages. It is known that convolutional neural networks (CNN) can usually be used to classify text data. However, the data collected in this case consists in numeric values. Therefore, an ANN is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "data = pd.read_csv('spambase.data')\n",
    "X = data.iloc[:, 0:57].values\n",
    "Y = data.iloc[:, 57].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Splitting data into training and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is split into training and testing sets with a 80-20 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling is used to scale all the features so that none is more important than another in the naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "Xtrain = sc.fit_transform(Xtrain)\n",
    "Xtest = sc.transform(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Keras library is used to create and execute the artificial neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. First configuration (5 x Relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first configuration is a five-layers ANN. The input dimension is 57 (as there are 57 dimensions) while the output dimension is one. The intermediate layers have 29 units (using the thumb rule (57+1)/2).\n",
    "\n",
    "The activation function is a 'relu' for each of the intermediate layer, except the output layer which has a 'sigmoid' one.\n",
    "\n",
    "The number of iterations is arbitrarily fixed at 100 while the batch size is 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3680/3680 [==============================] - 1s 253us/step - loss: 0.3539 - acc: 0.8326\n",
      "Epoch 2/100\n",
      "3680/3680 [==============================] - 0s 123us/step - loss: 0.2000 - acc: 0.9332\n",
      "Epoch 3/100\n",
      "3680/3680 [==============================] - 0s 116us/step - loss: 0.1806 - acc: 0.9391\n",
      "Epoch 4/100\n",
      "3680/3680 [==============================] - 0s 115us/step - loss: 0.1699 - acc: 0.9399\n",
      "Epoch 5/100\n",
      "3680/3680 [==============================] - 0s 118us/step - loss: 0.1612 - acc: 0.9429\n",
      "Epoch 6/100\n",
      "3680/3680 [==============================] - 0s 116us/step - loss: 0.1509 - acc: 0.9492\n",
      "Epoch 7/100\n",
      "3680/3680 [==============================] - 0s 135us/step - loss: 0.1465 - acc: 0.9535\n",
      "Epoch 8/100\n",
      "3680/3680 [==============================] - 0s 116us/step - loss: 0.1406 - acc: 0.9533\n",
      "Epoch 9/100\n",
      "3680/3680 [==============================] - 0s 116us/step - loss: 0.1354 - acc: 0.9546\n",
      "Epoch 10/100\n",
      "3680/3680 [==============================] - 0s 116us/step - loss: 0.1302 - acc: 0.9587\n",
      "Epoch 11/100\n",
      "3680/3680 [==============================] - 0s 123us/step - loss: 0.1270 - acc: 0.9573\n",
      "Epoch 12/100\n",
      "3680/3680 [==============================] - 0s 118us/step - loss: 0.1203 - acc: 0.9620\n",
      "Epoch 13/100\n",
      "3680/3680 [==============================] - 0s 125us/step - loss: 0.1179 - acc: 0.9617\n",
      "Epoch 14/100\n",
      "3680/3680 [==============================] - 0s 131us/step - loss: 0.1146 - acc: 0.9630\n",
      "Epoch 15/100\n",
      "3680/3680 [==============================] - 0s 117us/step - loss: 0.1090 - acc: 0.9679\n",
      "Epoch 16/100\n",
      "3680/3680 [==============================] - 0s 118us/step - loss: 0.1132 - acc: 0.9649\n",
      "Epoch 17/100\n",
      "3680/3680 [==============================] - 0s 122us/step - loss: 0.1066 - acc: 0.9663\n",
      "Epoch 18/100\n",
      "3680/3680 [==============================] - 0s 115us/step - loss: 0.1036 - acc: 0.9690\n",
      "Epoch 19/100\n",
      "3680/3680 [==============================] - 0s 116us/step - loss: 0.1013 - acc: 0.9682\n",
      "Epoch 20/100\n",
      "3680/3680 [==============================] - 0s 123us/step - loss: 0.0949 - acc: 0.9720\n",
      "Epoch 21/100\n",
      "3680/3680 [==============================] - 0s 114us/step - loss: 0.0967 - acc: 0.9707\n",
      "Epoch 22/100\n",
      "3680/3680 [==============================] - 0s 115us/step - loss: 0.0976 - acc: 0.9717\n",
      "Epoch 23/100\n",
      "3680/3680 [==============================] - 0s 119us/step - loss: 0.0925 - acc: 0.9717\n",
      "Epoch 24/100\n",
      "3680/3680 [==============================] - 0s 116us/step - loss: 0.0916 - acc: 0.9717\n",
      "Epoch 25/100\n",
      "3680/3680 [==============================] - 0s 113us/step - loss: 0.0946 - acc: 0.9731\n",
      "Epoch 26/100\n",
      "3680/3680 [==============================] - 0s 120us/step - loss: 0.0872 - acc: 0.9742\n",
      "Epoch 27/100\n",
      "3680/3680 [==============================] - 1s 139us/step - loss: 0.0843 - acc: 0.9734\n",
      "Epoch 28/100\n",
      "3680/3680 [==============================] - 0s 124us/step - loss: 0.0793 - acc: 0.9764\n",
      "Epoch 29/100\n",
      "3680/3680 [==============================] - 0s 132us/step - loss: 0.0829 - acc: 0.9739\n",
      "Epoch 30/100\n",
      "3680/3680 [==============================] - 0s 119us/step - loss: 0.0868 - acc: 0.9726\n",
      "Epoch 31/100\n",
      "3680/3680 [==============================] - 0s 129us/step - loss: 0.0757 - acc: 0.9764\n",
      "Epoch 32/100\n",
      "3680/3680 [==============================] - 0s 122us/step - loss: 0.0707 - acc: 0.9796\n",
      "Epoch 33/100\n",
      "3680/3680 [==============================] - 0s 128us/step - loss: 0.0764 - acc: 0.9747\n",
      "Epoch 34/100\n",
      "3680/3680 [==============================] - 0s 117us/step - loss: 0.0762 - acc: 0.9758\n",
      "Epoch 35/100\n",
      "3680/3680 [==============================] - 1s 141us/step - loss: 0.0745 - acc: 0.9753\n",
      "Epoch 36/100\n",
      "3680/3680 [==============================] - 1s 140us/step - loss: 0.0725 - acc: 0.9772\n",
      "Epoch 37/100\n",
      "3680/3680 [==============================] - 0s 116us/step - loss: 0.0698 - acc: 0.9761\n",
      "Epoch 38/100\n",
      "3680/3680 [==============================] - 0s 112us/step - loss: 0.0696 - acc: 0.9761\n",
      "Epoch 39/100\n",
      "3680/3680 [==============================] - 0s 114us/step - loss: 0.0651 - acc: 0.9783\n",
      "Epoch 40/100\n",
      "3680/3680 [==============================] - 0s 118us/step - loss: 0.0638 - acc: 0.9788\n",
      "Epoch 41/100\n",
      "3680/3680 [==============================] - 0s 114us/step - loss: 0.0644 - acc: 0.9791\n",
      "Epoch 42/100\n",
      "3680/3680 [==============================] - 0s 111us/step - loss: 0.0609 - acc: 0.9804\n",
      "Epoch 43/100\n",
      "3680/3680 [==============================] - 0s 118us/step - loss: 0.0573 - acc: 0.9793\n",
      "Epoch 44/100\n",
      "3680/3680 [==============================] - 0s 116us/step - loss: 0.0618 - acc: 0.9788\n",
      "Epoch 45/100\n",
      "3680/3680 [==============================] - 0s 113us/step - loss: 0.0552 - acc: 0.9818\n",
      "Epoch 46/100\n",
      "3680/3680 [==============================] - 0s 116us/step - loss: 0.0598 - acc: 0.9815\n",
      "Epoch 47/100\n",
      "3680/3680 [==============================] - 0s 118us/step - loss: 0.0602 - acc: 0.9810\n",
      "Epoch 48/100\n",
      "3680/3680 [==============================] - 0s 127us/step - loss: 0.0516 - acc: 0.9834\n",
      "Epoch 49/100\n",
      "3680/3680 [==============================] - 0s 123us/step - loss: 0.0571 - acc: 0.9807\n",
      "Epoch 50/100\n",
      "3680/3680 [==============================] - 0s 129us/step - loss: 0.0544 - acc: 0.9815\n",
      "Epoch 51/100\n",
      "3680/3680 [==============================] - 0s 124us/step - loss: 0.0525 - acc: 0.9826\n",
      "Epoch 52/100\n",
      "3680/3680 [==============================] - 1s 141us/step - loss: 0.0499 - acc: 0.9826\n",
      "Epoch 53/100\n",
      "3680/3680 [==============================] - 0s 136us/step - loss: 0.0464 - acc: 0.9842\n",
      "Epoch 54/100\n",
      "3680/3680 [==============================] - 1s 137us/step - loss: 0.0457 - acc: 0.9870\n",
      "Epoch 55/100\n",
      "3680/3680 [==============================] - 0s 131us/step - loss: 0.0473 - acc: 0.9842\n",
      "Epoch 56/100\n",
      "3680/3680 [==============================] - 0s 131us/step - loss: 0.0548 - acc: 0.9815\n",
      "Epoch 57/100\n",
      "3680/3680 [==============================] - 0s 134us/step - loss: 0.0433 - acc: 0.9856\n",
      "Epoch 58/100\n",
      "3680/3680 [==============================] - 0s 121us/step - loss: 0.0486 - acc: 0.9834\n",
      "Epoch 59/100\n",
      "3680/3680 [==============================] - 0s 119us/step - loss: 0.0538 - acc: 0.9821\n",
      "Epoch 60/100\n",
      "3680/3680 [==============================] - 0s 127us/step - loss: 0.0425 - acc: 0.9859\n",
      "Epoch 61/100\n",
      "3680/3680 [==============================] - 0s 105us/step - loss: 0.0420 - acc: 0.9851\n",
      "Epoch 62/100\n",
      "3680/3680 [==============================] - 0s 94us/step - loss: 0.0469 - acc: 0.9840\n",
      "Epoch 63/100\n",
      "3680/3680 [==============================] - 0s 96us/step - loss: 0.0454 - acc: 0.9840\n",
      "Epoch 64/100\n",
      "3680/3680 [==============================] - 0s 99us/step - loss: 0.0422 - acc: 0.9840\n",
      "Epoch 65/100\n",
      "3680/3680 [==============================] - 1s 149us/step - loss: 0.0501 - acc: 0.9834\n",
      "Epoch 66/100\n",
      "3680/3680 [==============================] - 0s 119us/step - loss: 0.0468 - acc: 0.9848\n",
      "Epoch 67/100\n",
      "3680/3680 [==============================] - 0s 118us/step - loss: 0.0459 - acc: 0.9856\n",
      "Epoch 68/100\n",
      "3680/3680 [==============================] - 0s 135us/step - loss: 0.0351 - acc: 0.9886\n",
      "Epoch 69/100\n",
      "3680/3680 [==============================] - 1s 143us/step - loss: 0.0370 - acc: 0.9859\n",
      "Epoch 70/100\n",
      "3680/3680 [==============================] - 1s 146us/step - loss: 0.0353 - acc: 0.9875\n",
      "Epoch 71/100\n",
      "3680/3680 [==============================] - 0s 134us/step - loss: 0.0333 - acc: 0.9883\n",
      "Epoch 72/100\n",
      "3680/3680 [==============================] - 1s 162us/step - loss: 0.0358 - acc: 0.9859\n",
      "Epoch 73/100\n",
      "3680/3680 [==============================] - 0s 131us/step - loss: 0.0349 - acc: 0.9880\n",
      "Epoch 74/100\n",
      "3680/3680 [==============================] - 1s 146us/step - loss: 0.0428 - acc: 0.9842\n",
      "Epoch 75/100\n",
      "3680/3680 [==============================] - 1s 148us/step - loss: 0.0424 - acc: 0.9853\n",
      "Epoch 76/100\n",
      "3680/3680 [==============================] - 1s 150us/step - loss: 0.0356 - acc: 0.9870\n",
      "Epoch 77/100\n",
      "3680/3680 [==============================] - 1s 148us/step - loss: 0.0339 - acc: 0.9867\n",
      "Epoch 78/100\n",
      "3680/3680 [==============================] - 1s 165us/step - loss: 0.0311 - acc: 0.9880\n",
      "Epoch 79/100\n",
      "3680/3680 [==============================] - 1s 144us/step - loss: 0.0338 - acc: 0.9889\n",
      "Epoch 80/100\n",
      "3680/3680 [==============================] - 1s 170us/step - loss: 0.0383 - acc: 0.9859\n",
      "Epoch 81/100\n",
      "3680/3680 [==============================] - 1s 149us/step - loss: 0.0346 - acc: 0.9864\n",
      "Epoch 82/100\n",
      "3680/3680 [==============================] - 0s 132us/step - loss: 0.0317 - acc: 0.9878\n",
      "Epoch 83/100\n",
      "3680/3680 [==============================] - 1s 157us/step - loss: 0.0359 - acc: 0.9872 0s - loss: 0.0320 - acc: 0\n",
      "Epoch 84/100\n",
      "3680/3680 [==============================] - 0s 112us/step - loss: 0.0372 - acc: 0.9856\n",
      "Epoch 85/100\n",
      "3680/3680 [==============================] - 0s 120us/step - loss: 0.0260 - acc: 0.9883\n",
      "Epoch 86/100\n",
      "3680/3680 [==============================] - 0s 116us/step - loss: 0.0312 - acc: 0.9886\n",
      "Epoch 87/100\n",
      "3680/3680 [==============================] - 0s 119us/step - loss: 0.0304 - acc: 0.9889\n",
      "Epoch 88/100\n",
      "3680/3680 [==============================] - 0s 114us/step - loss: 0.0337 - acc: 0.9875\n",
      "Epoch 89/100\n",
      "3680/3680 [==============================] - 0s 120us/step - loss: 0.0262 - acc: 0.9897\n",
      "Epoch 90/100\n",
      "3680/3680 [==============================] - 0s 115us/step - loss: 0.0302 - acc: 0.9889\n",
      "Epoch 91/100\n",
      "3680/3680 [==============================] - 0s 120us/step - loss: 0.0315 - acc: 0.9867\n",
      "Epoch 92/100\n",
      "3680/3680 [==============================] - 0s 116us/step - loss: 0.0288 - acc: 0.9889\n",
      "Epoch 93/100\n",
      "3680/3680 [==============================] - 0s 114us/step - loss: 0.0296 - acc: 0.9880\n",
      "Epoch 94/100\n",
      "3680/3680 [==============================] - 0s 114us/step - loss: 0.0243 - acc: 0.9894\n",
      "Epoch 95/100\n",
      "3680/3680 [==============================] - 0s 115us/step - loss: 0.0379 - acc: 0.9851\n",
      "Epoch 96/100\n",
      "3680/3680 [==============================] - 0s 113us/step - loss: 0.0266 - acc: 0.9891\n",
      "Epoch 97/100\n",
      "3680/3680 [==============================] - 0s 120us/step - loss: 0.0226 - acc: 0.9899\n",
      "Epoch 98/100\n",
      "3680/3680 [==============================] - 0s 119us/step - loss: 0.0283 - acc: 0.9880\n",
      "Epoch 99/100\n",
      "3680/3680 [==============================] - 0s 135us/step - loss: 0.0266 - acc: 0.9883\n",
      "Epoch 100/100\n",
      "3680/3680 [==============================] - 1s 158us/step - loss: 0.0302 - acc: 0.9902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b083c9d6a0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(units = 29, kernel_initializer = 'uniform', activation = 'relu', input_dim = 57))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units = 29, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the third hidden layer\n",
    "classifier.add(Dense(units = 29, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the fourth hidden layer\n",
    "classifier.add(Dense(units = 29, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the fifth hidden layer\n",
    "classifier.add(Dense(units = 29, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(Xtrain, Ytrain, batch_size = 10, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has been trained on train data, it can be assessed by using test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(Xtest)\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix as well as the accuracy is computed and printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[528  20]\n",
      " [ 26 346]]\n",
      "0.95\n"
     ]
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(Ytest, y_pred)\n",
    "print(cm)\n",
    "print((cm[0][0]+cm[1][1])/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
